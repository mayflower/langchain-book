# Der Anfang mit LangChain

Harrison Chase hat das LangChain Projekt im Oktober 2022 begonnen, und während ich dieses Buch im Februar 2023 schreibe, hat das GitHub-Repository für LangChain <https://github.com/hwchase17/langchain> 171 Beitragende.

[LangChain](https://langchain.readthedocs.io/en/latest/index.html) ist ein Framework zur Erstellung von Anwendungen mit großen Sprachmodellen (Large Language Models, LLMs). Dies geschieht durch Verkettung verschiedener Komponenten. Einige der Anwendungen von LangChain sind Chatbots, generative Fragebeantwortung, Zusammenfassungen, datenunterstützte Generierung und mehr. LangChain kann Zeit bei der Erstellung von Chatbots und anderen Systemen sparen, indem es eine Standardschnittstelle für Ketten, Agenten und Speicher schafft, ebenso wie die Integration mit anderen Tools und End-to-End-Beispiele. Eine Abfolge von Aufrufen (von LLMs und anderen Programmen, Clouddiensten etc.). die einen einzelnen Aufruf der API eines LLM hinausgehen, bezeichnen wir als "Kette" ("Chain"). LangChain bietet eine Standardschnittstelle für Ketten, viele Integrationen mit anderen Tools und Ende-zu-Ende Ketten für gängige Anwendungen. Oft findest du bereits existierende Ketten, die die Anforderungen deiner Anwendungen erfüllen.

Man kann zum Beispiel eine Kette erstellen, die Benutzereingaben entgegennimmt, sie mit einem PromptTemplate formatiert und dann die formatierte Antwort an ein Large Language Model (LLM) zur Verarbeitung weitergibt.

LLMs sind sehr allgemein gehalten, was bedeutet, dass sie zwar viele Aufgaben effektiv ausführen können, aber oft nicht direkt spezifische Antworten auf Fragen oder Aufgaben geben können, wenn diese tieferes Domänenwwissen oder Expertise benötigen. LangChain bietet eine Standardschnittstelle für Agenten, eine Bibliothek von Agenten aus der man auswählen kann, ebenso wie Beispiele für Ende-zu-Ende-Agenten.

LangChain Speicher ermöglicht es, den Zustand zwischen Aufrufen einer Kette oder eines Agenten vorzuhalten. LangChain bietet eine standardisierte Schnittstelle für Speicher, eine Sammlung von Speicherimplementierungen sowie Beispiele für Ketten oder Agenten, die Speicher verwenden. LangChain stellt eine große Sammlung von allgemeinen Werkzeugen zur Verwendung in eigenen Anwendungen zur Verfügung. Ketten gehen über einen einzelnen LLM-Aufruf hinaus und sind Abfolgen von Aufrufen (an ein LLM oder irgendein Dienstprogramm). LangChain bietet ebenfalls eine Standardschnittstelle für Ketten, eine Vielzahl an Integrationen mit anderen Werkzeugen sowie Ende-zu-Ende-Ketten für gängige Anwendungen.

LangChain kann mit einem oder mehreren Modellanbietern, Datenspeichern, APIs, usw. integriert werden. Es kann für ausführliche Frage-und-Antwort Chatsitzungen, API-Interaktion oder das Ausführen von Aktionen verwendet werden. Über eine natürlichsprachliche API-Schnittstelle kann LangChain in die Zapier-Plattform integriert werden (es gibt ein ganzes Kapitel zu den Zapier-Integrationen).

## Installation der erforderlichen Pakete

Für die Beispiele in diesem Buch empfiehlt es sich, eine neue Anaconda- oder sonstige Python-Umgebung zu erstellen und folgendes zu installieren:

{format: console}
```
pip install langchain llama_index openai
pip install kor pydrive pandas rdflib
pip install google-search-results SPARQLWrapper
```

Für den Rest dieses Kapitels werden wir das Unterverzeichnis **langchain_- getting_started** und im nächsten Kapitel **llama-index_case_study** aus dem GitHub-Repository für dieses Buch verwenden.

## Ein neues LangChain-Projekt anlegen

Einfache LangChain-Projekte bestehen oft nur aus einem einfachen Python-Skript. Wenn dir beim Lesen dieses Buches ein Beispiel interessant oder nützlich erscheint, schlage ich vor, die requirements.txt und die Python-Quelldateien in ein neues Verzeichnis zu kopieren und dein eigenes privates GitHub-Repository für deine Arbeit anzulegen. Bitte mache "deinen Code" aus den Beispielen in diesem Buch, verwende also frei jeden Code oder jede Idee, die du hier findest.

## Grundlegende Anwendung und Beispiele

Ich versuche, das Material in diesem Buch unabhängig und ohne externe Referenzen nutzbar zu gestalten. Du solltest aber auch die ausgezeichnete [Dokumentation](https://python.langchain.com/en/latest/getting_started/getting_started.html) und die detaillierten Anleitungen für Prompts, Chat, das Laden von Dokumenten, Indizes usw. nutzen.

Beachte bitte beim Durcharbeiten der Beipiele, wie die ChatGPT-Webanwendung arbeitet: Du gibst Text ein und erhältst Antworten. Die Art und Weise, wie du ChatGPT "ansprichst", ist offensichtlich wichtig, um nützliche Antworten zu erhalten. In Codebeispielen automatisieren und formalisieren wir diesen Prozess.

Du musst ein LLM auswählen, das du verwenden möchtest. In der Regel wählen wir die GPT-3.5-API von OpenAI, da sie universell einsetzbar und viel kostengünstiger ist als die früheren Modell-APIs von OpenAI. Du musst dich [anmelden](https://platform.openai.com/account/api-keys), um einen API-Key zu erhalten und diesen dann als Umgebungsvariable festlegen:

{format: console}
```
export OPENAI_API_KEY="DEIN API_KEY HIER"
```

Sowohl die **openai** als auch die **langchain** Bibliotheken verwenden diese Umgebungsvariable. Schauen wir uns ein paar einfache Beispiele in einer Python-REPL an. Wir beginnen mit der Textvorhersage-API von OpenAI:

{format: console}
```
$python
>>> from langchain.llms import OpenAI
>>> llm = OpenAI(temperature=0.8)
>>> s = llm("John got into his new sportscar, and he drove it")
>>> s
'to work\n\nJohn started up his new sportscar and drove it to work. He had a huge smile on his face as he drove, excited to show off his new car to his colleagues. The wind blowing through his hair, and the sun on his skin, he felt a sense of freedom and joy as he cruised along the road. He arrived at work in no time, feeling refreshed and energized.'
>>> s = llm("John got into his new sportscar, and he drove it")
>>> s
"around town\n\nJohn drove his new sportscar around town, enjoying the feeling of the wind in his hair. He stopped to admire the view froma scenic lookout, and then sped off to the mall to do some shopping. On the way home, he took a detour down a winding country road, admiring the scenery and enjoying the feel of the car's powerful engine. By the time he arrived back home, he had a huge smile on his face."
```

Beachte, wie wir bei zweifacher Ausführung des gleichen Eingabeprompts unterschiedliche Ergebnisse erhalten. Durch das Erhöhen der Temperatur in Zeile 3 wird die Zufälligkeit erhöht.

Unser nächstes Beispiel befindet sich in der Quelldatei **directions_template.py** und verwendet die Klasse **PromptTemplate**. Ein PromptTemplate ist ein reproduzierbarer Weg zu einem Prompts. Es enthält einen Textstring ("die Vorlage"), der eine Reihe von Parametern entgegennehmen und daraus einen Prompt erzeugen kann. Das PromptTemplate kann Anweisungen für das Sprachmodell, "few-shot" Beispiele zur Verbesserung der Antwort des Modells oder spezifische Fragen, die das Modell beantworten soll enthalten.

{format: python}
![](code/03/03-directions_template.py)

Man könnte einfach Python-Code zur Manipulation von Zeichenketten schreiben, um ein Prompt zu erstellen, aber die Verwendung der Utilityklasse **PromptTemplate** ist besser lesbar und funktioniert mit einer beliebigen Anzahl von Eingabevariablen.

Das Ergebnis ist:

{format: console}
```
$python directions_template.py

    How do I get to the store?:

    To get to the store, you will need to use a mode of transportation such as walking, driving, biking, or taking public transportation. Depending on the location of the store, you may need to look up directions or maps to determine the best route to take.

    How do I hang a picture on the wall?:

    1. Find a stud in the wall, or use two or three wall anchors for heavier pictures.
    2. Measure and mark the wall where the picture hanger will go.
    3.Pre-drill the holes and place wall anchors if needed.
    4.Hammer the picture hanger into the holes.
    5.Hang the picture on the picture hanger.
```

Das nächste Beispiel in der Datei **country_information.py** ist von einem Beispiel in der LangChain-Dokumentation abgeleitet. Hier verwenden wir ein **PromptTemplate**, das das gewünschte Ausgabemuster für das LLM enthält.

{format: python}
![](code/03/03-country_information.py)

Du kannst die ChatGPT-Weboberfläche verwenden, um mit Prompts zu experimentieren. Wenn du ein Muster gefunden hast, das gut funktioniert, schreibe es in ein Python-Skript wie im letzten Beispiel, ändere aber die Daten, die du übergibst in der **PromptTemplate**-Instanz.

Die Ausgabe des letzten Beispiels ist:

{format: console}
```
$ python country_information.py

Processing Canada:
Creating prompt...

Predict the capital and population of a country.

Country: Canada
Capital:
Population:

Capital: Ottawa
Population: 37.058.856 (as of July 2020)

Processing Germany:

Predict the capital and population of a country.

Country: Germany
Capital:
Population:

Capital: Berlin
Population: 83,02 million (est.2019)
```

## Embeddings erstellen

Wir beziehen uns auf die [Dokumentation zu Embeddings von LangChain](https://python.langchain.com/en/latest/reference/modules/embeddings.html). Mit einer Python REPL können wir ausprobieren, wie Text- zu Vektorraum-Embeddings aussehen könnten:

{format: console}
```
$ python
Python 3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> from langchain.embeddings import OpenAIEmbeddings
>>> embeddings = OpenAIEmbeddings()
>>> text = "Mary has blond hair and John has brown hair. Mary lives in town and John lives in the country."
>>> doc_embeddings = embeddings.embed_documents([text])
>>> doc_embeddings[[0.007727328687906265, 0.0009025644976645708, -0.0033224383369088173, -0.01794492080807686, -0.017969949170947075, 0.028506645932793617, -0.013414892368018627, 0.0046676816418766975, -0.0024965214543044567, -0.02662956342101097,...]]
>>> query_embedding = embeddings.embed_query("Does John live in the city?")
>>> query_embedding[0.028048301115632057, 0.011499025858938694, -0.00944007933139801, -0.020809611305594444, -0.023904507979750633, 0.018750663846731186, -0.01626438833773136, 0.018129095435142517,...]
>>>
```

Beachte, dass **doc_embeddings** eine Liste ist, bei der jedes Listenelement das Embedding für ein Textdokument aus der Eingabe ist.

Das **query_embedding** ist ein einzelnes Embedding. Lies dir bitte die oben verlinkte Dokumentation zu Embeddings durch. 

Wir werden Vektordatenbanken verwenden, um berechnete Embeddings für zukünftige Verwendungszwecke zu speichern. Im nächsten Kapitel werden wir ein Beispiel einer Suche in einer Dokumentendatenbank mittels LangChain und Llama-Index betrachten. 

## Verwendung von LangChain Vektor Stores zum Durchsuchen von Dokumenten

Hier beziehen wir uns auf die Dokumentation zu [LangChain Vector Stores](https://python.langchain.com/en/latest/reference/modules/vectorstores.html). Du musst ein paar Bibliotheken installieren:

{format: console}
```
pip install chroma
pip install chromadb
pip install unstructured pdf2image pytesseract
```

Das Beispielskript ist **doc_search.py**:

{format: python}
![](code/03/03-doc_search.py)

Die Klasse DirectoryLoader ist hilfreich für das Laden eines Verzeichnisses voller Eingabedokumente. In diesem Beispiel haben wir festgelegt, dass wir nur Textdateien verarbeiten wollen, aber das Dateimuster hätte auch PDF-Dateien usw. enthalten können.

Das Ergebnis ist:

{format: console}
```
$ python doc_search.py
Created a chunk of size 1055, which is longer than the specified 1000
Running Chroma using direct local API.
Using DuckDB in-memory for database. Data will be transient.
Query: What kinds of equipment are in a chemistry laboratory?
Answer: A chemistry lab would typically include glassware, such as beakers, flasks, and testtubes, as well as other equipment such as scales, Bunsenburners, and thermometers.
Query: What is Austrian School of Economics?
Answer: The Austrian School is a school of economic thought that emphasizes the spontaneous organizing power of the price mechanism. Austrians hold that the complexity of subjective human choices makes mathematical modelling of the evolving market extremely difficult and advocate a "laissezfaire" approach to the economy. Austrian School economists advocate the strict enforcement of voluntary contractual agreements between economic agents, and hold that commercial transactions should be subject to the smallest possible imposition of forces they consider to be (in particular the smallest possible amount of government intervention). The Austrian School derives its name from its predominantly Austrian founders and early supporters, including Carl Menger, Eugen von Bohm-Bawerk and Ludwig von Mises.
Query: Why do people engage in sports?
Answer: People engage in sports for leisure and entertainment, as well as for physical exercise and athleticism.
Query: What is the effect of bodychemistry on exercise?
Answer: Body chemistry can affect the body's response to exercise, as certain hormones and enzymes produced by the body can affect the energy levels and muscleperformance. Chemicals in the body, such as adenosinetriphosphate(ATP) and urea, can affect the body's energy production and muscle metabolism during exercise. Additionally,the body's levels of electrolytes, vitamins, and minerals can affect exercise performance.
Exiting: Cleaning up .chroma directory
```

## Zusammenfassung des Überblicks

Für den Rest dieses Buches werden wir weiter LangChain verwenden, ebenso wie die LlamaIndex-Bibliothek, die wir im nächsten Kapitel vorstellen.

Ich behandle nur die Teilmenge von LangChain, die ich in meinen Projekten in diesem Buch verwende. Lies unbedingt auch die LangChain-Dokumentation und schau dir die öffentlichen LangChain-Ketten an, die Benutzer auf
[Langchain-hub](https://github.com/hwchase17/langchain-hub) geschrieben haben.
