# Nutzung von DBPedia und Wikidata als Wissensquellen

[DBPedia](https://www.dbpedia.org)[^1] und [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page)[^2] sind öffentliche Knowledge Graphs (KGs), die Daten als [Resource Description Framework (RDF)](https://www.w3.org/RDF/)[^3] speichern und auf die über die [SPARQL Query Language for RDF](https://www.w3.org/TR/rdf-sparql-query/)[^4] zugegriffen werden kann. Die Beispiele für dieses Projekt befinden sich im [GitHub-Repository für dieses Buch](https://github.com/mark-watson/langchain-book-examples)[^5] im Verzeichnis **kg_search**.

Ich werde hier nicht viel Zeit auf RDF und SPARQL verwenden. Stattdessen bitte ich dich, das einführende Kapitel **Linked Data, the Semantic Web, and Knowledge Graphs** in meinem Buch [A Lisp Programmer Living in Python-Land: The Hy Programming Language](https://leanpub.com/hy-lisp-python/read)[^6] online zu lesen.

Wie wir im letzten Kapitel gesehen haben, ist ein Knowledge Graph (den ich oft mit KG abkürze) eine Graphdatenbank, die mittels eines Schemas Typen (sowohl Objekte als auch Beziehungen zwischen Objekten) und Eigenschaften, die Eigenschaftswerte mit Objekten verbinden, definiert. Der Begriff "Knowledge Graph" ist ein allgemeiner Begriff der manchmal aber auch auf den spezifischen Knowledge Graph, der bei Google verwendet wird, meint, und mit dem ich während meiner Tätigkeit dort im Jahr 2013 gearbeitet habe. Hier verwenden wir KG, um auf die allgemeine Technologie der Speicherung von Wissen in Graphdatenbanken zu verweisen.

DBPedia und Wikidata sind ähnlich, jedoch mit einigen wichtigen Unterschieden. Hier ist eine Zusammenfassung einiger Gemeinsamkeiten und Unterschiede zwischen DBPedia und Wikidata:

- beide Projekte wollen strukturierte Daten aus Wikipedia in verschiedenen Formaten und Sprachen bereitstellen. Wikidata enthält auch Daten aus anderen Quellen, daher enthält es mehr Daten und mehr Sprachen.
- beide Projekte verwenden RDF als gemeinsames Datenmodell und SPARQL als Abfragesprache.
- DBPedia extrahiert Daten aus den Infoboxen der Wikipedia-Artikel, während Wikidata Daten aus Benutzereingeben als auch von automatisierten Bots gelieferte sammelt.
- Wikidata benötigt Quellen für seine Daten, während DBPedia dies nicht tut.
- DBPedia ist in der Semantic-Web- und Linked-Open-Data-Gemeinschaft populärer, während Wikidata stärker in die Wikimedia-Projekte integriert ist.

Zum letzten Punkt: Ich persönlich bevorzuge DBPedia, wenn ich mit dem semantischen Web und verknüpften Daten experimentiere, vor allem, weil DBPedia-URIs für Menschen lesbar sind, während Wikidata-URIs abstrakt sind. Die folgenden URIs repräsentieren die Stadt Sedona (Arizona), in der ich lebe:

- DBPedia: https://dbpedia.org/page/Sedona,_Arizona
- Wikidata: https://www.wikidata.org/wiki/Q80041

In RDF schließen wir URIs in spitze Klammern wie **&lt; https://www.wikidata.org/wiki/Q80041 &gt;** ein.

Wenn du das Kapitel über RDF und SPARQL in meinem bereits erwähnten Buch gelesen hast, dann weißt du, dass RDF-Daten durch Tripel dargestellt werden, bei denen jeder Teil benannt ist:

- Subjekt
- Eigenschaft
- Objekt

Wir werden uns in diesem Kapitel zwei ähnliche Beispiele ansehen, eines mit DBPedia und eines mit Wikidata. Beide Dienste verfügen über SPARQL-Endpunkt-Webanwendungen, die du für die Erkundung beider KGs verwenden kannst. Wir schauen uns die DBPedia-Weboberfläche später an. Hier ist die Wikidata-Webschnittstelle:

![](images/wikidata.png)

In dieser SPARQL-Abfrage steht das Präfix **wd:** für Wikidata-Daten, während das Präfix **wdt:** für Wikidata-Typ (oder Eigenschaft) steht. Das Präfix **rdfs:** steht für RDF Schema.

[^1]: https://www.dbpedia.org
[^2]: https://www.wikidata.org/wiki/Wikidata:Main_Page
[^3]: https://www.w3.org/RDF/
[^4]: https://www.w3.org/TR/rdf-sparql-query/
[^5]: https://github.com/mark-watson/langchain-book-examples
[^6]: https://leanpub.com/hy-lisp-python/read

## DBPedia als Datenquelle

DBPedia ist ein Communityprojekt, das strukturierte Inhalte aus Wikipedia extrahiert und sie im Web als Knowledge Graph (KG) zur Verfügung stellt. Der KG ist eine wertvolle Ressource für Forscher und Entwickler, die auf strukturierte Daten aus Wikipedia zugreifen müssen. Durch die Verwendung von SPARQL-Abfragen an DBPedia als Datenquelle können wir eine Vielzahl von Anwendungen schreiben, darunter natürliche Sprachverarbeitung, maschinelles Lernen und Datenanalyse. Wir demonstrieren die Effektivität von DBPedia als Datenquelle an mehreren Beispielen, die den Einsatz in realen Anwendungen veranschaulichen. Nach meiner Erfahrung ist DBPedia eine wertvolle Ressource für Forscher und Entwickler, die auf strukturierte Daten aus Wikipedia zugreifen müssen.

Im Allgemeinen beginnst du Projekte, die DBPedia verwenden, indem du die verfügbaren Daten mit der Web-App https://dbpedia.org/sparql untersuchst, die in diesem Screenshot zu sehen ist:

![](images/dbpedia.png)

Die folgende Datei **dbpedia_generate_rdf_as_nt.py** zeigt Python-Code, um eine SPARQL-Abfrage an DBPedia zu stellen und die Ergebnisse als RDF-Tripel im NT-Format in einer lokalen Textdatei zu speichern:

{format: python}
![](code/06/06-dbpedia_generate_rdf_as_nt.py)

Hier ist die Ausgabe der Ergebnisse dieses Skripts (die meisten Ausgaben werden nicht angezeigt, und sie wurden manuell an die Seitenbreite angepasst):

{format: console}
```
$ python generate_rdf_as_nt.py
results:
<http://dbpedia.org/resource/Ethiopia>
  <http://www.w3.org/2000/01/rdf-schema#label>
  "Ethiopia"@en .
<http://dbpedia.org/resource/Valentin_Alsina,_Buenos_Aires>
   <http://www.w3.org/2000/01/rdf-schema#label>
   "Valentin Alsina, Buenos Aires"@en .
 <http://dbpedia.org/resource/Davyd-Haradok>
   <http://dbpedia.org/ontology/country>
   <http://dbpedia.org/resource/Belarus> .
 <http://dbpedia.org/resource/Davyd-Haradok>
   <http://www.w3.org/2000/01/rdf-schema#label>
   "Davyd-Haradok"@en .
 <http://dbpedia.org/resource/Belarus>
   <http://www.w3.org/2000/01/rdf-schema#label>
   "Belarus"@en .
```

Diese Ausgabe wurde in eine lokale Datei **sample.nt** geschrieben. Ich habe dieses Beispiel in zwei separate Python-Skripte aufgeteilt, weil ich dachte, dass es für dich, lieber Leser, das Experimentieren einfacher wäre, wenn du den Abruf von RDF-Daten und ihre Nutzung mittels eines LLMs getrennt hältst. In der Praxis wirst du vermutlich KG-Abfragen mit semantischer Analyse kombinieren wollen.

Dieses Codebeispiel demonstriert die Verwendung des **GPTSimpleVectorIndex** zur Abfrage von RDF-Daten zum Abruf von Informationen über Länder. Die Funktion **download_loader** lädt Datenimporteure anhand ihres Namens als Zeichenkette. Es ist zwar nicht typsicher, eine Python-Klasse über den Namen einer Zeichenkette zu laden, aber wenn du den Namen der zu ladenden Klasse beim Aufruf von **download_loader** falsch schreibst, wird ein Python **ValueError("Loader class name not found in library")** Fehler ausgegeben. Die Klasse GPTSimpleVectorIndex stellt eine Indexdatenstruktur dar, die zum effizienten Suchen und Abrufen von Informationen aus den RDF-Daten verwendet werden kann. Dies ist vergleichbar mit anderen LlamaIndex-Vektorindextypen für unterschiedliche Arten von Datenquellen.

Hier ist das Skript **dbpedia_rdf_query.py**:

{format: python}
![](code/06/06-dbpedia_rdf_query.py)

Hier das Ergebnis:

{format: console}
```
$ python rdf_query.py
INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens
INFO:root:> [build_index_from_documents] Total embedding token usage: 761 tokens
INFO:root:> [query] Total LLM token usage: 921 tokens
INFO:root:> [query] Total embedding token usage: 12 tokens
['Argentina', 'French Polynesia', 'Democratic Republic of the Congo', 'Benin', 'Ethiopia', 'Australia', 'Uzbekistan', 'Tanzania', 'Albania', 'Belarus', 'Vanuatu', 'Armenia', 'Syria', 'Andorra', 'Venezuela', 'France', 'Vietnam', 'Azerbaijan']

This is a list of all the countries mentioned in the context information. All of the countries are listed in the context information, so this list is complete.
```

Warum werden nur 18 Länder aufgelistet? Im Skript für die SPARQL Suche auf DBPedia hatten wir am Ende der Suche ein Statement **LIMIT 50**. Deshalb wurden nur 50 RDF Tripel in die Datei **sample.net** geschrieben, die nur Daten für 18 Länder enthalten.

## Wikidata als Datenquelle

Die Abfrage von Wikidata gestaltet sich im Vergleich zu DBPedia etwas schwieriger. Schauen wir uns noch einmal an, wie ich Informationen über meine Heimatstadt Sedona Arizona erhalte.

Bei diesem Beispiels habe ich mit SPARQL-Abfragen experimentiert, indem ich die [Wikidata SPARQL Web-App](https://query.wikidata.org)[^7] verwendet habe.

Zuerst suchen wir mit der Wikidata-Webanwendung RDF-Einträge, bei denen der Wert für "object" "Sedona" ist.

{format: console}
```
select * where {
  ?s ?p "Sedona"@en
} LIMIT 30
```

Zuerst schreiben wir ein Hilfsprogramm, um den Prompttext für den Namen einer Entität (z.B. den Namen einer Person, eines Ortes usw.) zu erzeugen. Das passiert in der Datei **wikidata_generate_prompt_text.py**:

[^7]: https://query.wikidata.org

{format: python}
![](code/06/06-wikidata_generate_prompt_text.py)

Dieses Hilfsprogramm erledigt den größten Teil der Arbeit, um den Prompt Text für eine Entität zu erhalten.

Die Klasse **GPTTreeIndex** ähnelt den anderen Indexklassen von LlamaIndex. Diese Klasse baut einen baumbasierten Index der Eingabetexte auf, um Informationen basierend auf der Eingabefrage abzurufen. In LlamaIndex wird ein **GPTTreeIndex** verwendet, um die Kind-Knoten auszuwählen, an die die Anfrage gesendet werden soll. Ein **GPTKeywordTableIndex** vergleicht Schlüsselwörter und ein **GPTVectorStoreIndex** nutzt Kosinusähnlichkeit in den Embeddings. Die Wahl der zu verwendenden Indexklasse hängt davon ab, wie viel Text indiziert werden soll, wie granular der Inhalt des Textes ist und ob man Zusammenfassung wünscht.

**GPTTreeIndex** ist auch effizienter als **GPTSimpleVectorIndex**, da er eine Baumstruktur zur Datenspeicherung verwendet. Dies ermöglicht ein schnelleres Suchen und Abrufen von Daten im Vergleich zu einer linearen Listenindexklasse wie **GPTSimpleVectorIndex**.

Der LlamaIndex-Code ist relativ einfach im Skript **wikidata_query.py** implementiert:

{format: python}
![](code/06/06-wikidata_query.py)

Hier ist die Testausgabe (einige Zeilen wurden entfernt):

{format: console}
```
$ python wikidata_query.py
Total LLM token usage: 162 tokens
INFO:llama_index.token_counter.token_counter:> [build_index_from_documents] INFO:llama_index.indices.query.tree.leaf_query:> Starting query: WhatisSedona?
INFO:llama_index.token_counter.token_counter:> [query]Total LLM token usage: 154 tokens
Sedona: Sedona is a city in the United States located in the counties of Yavapai and Coconino, Arizona. It is also the title of a 2012 film, a company, and a 2015 single by Houndmouth.

Total LLM token usage: 191 tokens
INFO:llama_index.indices.query.tree.leaf_query:> Starting query: What is California?
California: California is a U.S. state in the UnitedStates of America.

Total LLM token usage: 138 tokens
INFO:llama_index.indices.query.tree.leaf_query:> Starting query: When was Bill Clinton president?
Bill Clinton: Bill Clinton was the 42nd President of  the United States from 1993 to 2001.

Total LLM token usage: 159 tokens
INFO:llama_index.indices.query.tree.leaf_query:> Starting query: Who is Donald Trump?
Donald Trump: Donald Trump is the 45th President of the United States, serving from 2017 to 2021.
```
